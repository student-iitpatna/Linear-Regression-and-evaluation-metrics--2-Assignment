{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P97dioaUhNek"
   },
   "source": [
    "# **Theoretical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtWbW5Y7hXU7"
   },
   "outputs": [],
   "source": [
    "### Q.1) What does R-squared represent in a regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccjJ499whivL"
   },
   "outputs": [],
   "source": [
    "ans)  R-squared represents the proportion of variance in the dependent variable that's explained by the independent variables, ranging from 0 to 1. A value of 0.75 means the model explains 75% of the variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzkX8Q09hobb"
   },
   "outputs": [],
   "source": [
    "### Q.2) What are the assumptions of linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyL757cLhr7j"
   },
   "outputs": [],
   "source": [
    "ans) The key assumptions are:\n",
    "\n",
    "1) Linearity: X and Y have a linear relationship\n",
    "2) Independence of observations\n",
    "3) Homoscedasticity: Constant variance of residuals\n",
    "4) Normality of residuals\n",
    "5) No multicollinearity among independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gs8oFZeghsa8"
   },
   "outputs": [],
   "source": [
    "### Q.3) What is the difference between R-squared and Adjusted R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWWdOn6DhvfL"
   },
   "outputs": [],
   "source": [
    "ans)   While R-squared always increases with added variables, Adjusted R-squared penalizes unnecessary variables and can decrease, making it better for model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKwLDOe2hxzL"
   },
   "outputs": [],
   "source": [
    "### Q.4) Why do we use Mean Squared Error (MSE)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MX9e6JTahx9b"
   },
   "outputs": [],
   "source": [
    "ans) MSE is used because it:\n",
    "\n",
    "1) Measures average squared differences between predictions and actuals\n",
    "2) Makes all errors positive through squaring\n",
    "3) Penalizes larger errors more heavily\n",
    "4) Is differentiable for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQq4noBohyEj"
   },
   "outputs": [],
   "source": [
    "### Q.5) What does an Adjusted R-squared value of 0.85 indicate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6HngyMfhyIL"
   },
   "outputs": [],
   "source": [
    "ans)  It indicates that 85% of the variance in the dependent variable is explained by the independent variables, accounting for the number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWwqOvAYhyLr"
   },
   "outputs": [],
   "source": [
    "### Q.6) How do we check for normality of residuals in linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CGVbaJhhyPT"
   },
   "outputs": [],
   "source": [
    "ans)  Through:\n",
    "\n",
    "1) Q-Q plots\n",
    "2) Histograms of residuals\n",
    "3) Shapiro-Wilk test\n",
    "4) Kolmogorov-Smirnov test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ngb0A08RhySk"
   },
   "outputs": [],
   "source": [
    "### Q.7) What is multicollinearity, and how does it impact regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_Y6OSquhyWM"
   },
   "outputs": [],
   "source": [
    "ans)  Multicollinearity is high correlation between independent variables. It causes:\n",
    "\n",
    "Unstable coefficient estimates\n",
    "Increased standard errors\n",
    "Difficulty in determining variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_6bjDBShyaD"
   },
   "outputs": [],
   "source": [
    "### Q.8) What is Mean Absolute Error (MAE)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRWskN7Fhydz"
   },
   "outputs": [],
   "source": [
    "ans) MAE is the average absolute difference between predicted and actual values. It's more robust to outliers than MSE and maintains the original unit of measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0DcUdfmhyhz"
   },
   "outputs": [],
   "source": [
    "### Q.9) What are the benefits of using an ML pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qa_S4mx-hylV"
   },
   "outputs": [],
   "source": [
    "ans)  ML pipelines provide:\n",
    "\n",
    "1) Standardized workflow\n",
    "2) Automated preprocessing\n",
    "3) Reproducibility\n",
    "4) Easier deployment and maintenance\n",
    "5) Reduced human error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_6G9zmkhyoz"
   },
   "outputs": [],
   "source": [
    "### Q.10) Why is RMSE considered more interpretable than MSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeDSwx_Bhysq"
   },
   "outputs": [],
   "source": [
    "ans) RMSE is in the same units as the target variable, while MSE is squared units. This makes RMSE easier to understand in context of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVUCpe4NhywM"
   },
   "outputs": [],
   "source": [
    "### Q.11) What is pickling in Python, and how is it useful in ML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bglVLBVkhyzj"
   },
   "outputs": [],
   "source": [
    "ans) Pickling is serializing Python objects to save them to files. In ML, it's used to:\n",
    "\n",
    "1) Save trained models\n",
    "2) Share models between systems\n",
    "3) Preserve preprocessing transformations\n",
    "4) Store model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4R-m57qhy3L"
   },
   "outputs": [],
   "source": [
    "### Q.12) What does a high R-squared value mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYvJpqjGhy6j"
   },
   "outputs": [],
   "source": [
    "ans) A high R-squared indicates the model explains a large portion of variance in the dependent variable, suggesting good fit. However, it could also indicate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMj6Yn_nhy90"
   },
   "outputs": [],
   "source": [
    "### Q.13) What happens if linear regression assumptions are violated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgFT6r3khzBS"
   },
   "outputs": [],
   "source": [
    "ans) Violations can lead to:\n",
    "\n",
    "1) Biased coefficients\n",
    "2) Incorrect standard errors\n",
    "3) Unreliable p-values\n",
    "4) Poor model generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fu2BJYtwhzE7"
   },
   "outputs": [],
   "source": [
    "### Q.14) How can we address multicollinearity in regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9m4RKlsSiL7j"
   },
   "outputs": [],
   "source": [
    "ans)  Methods include:\n",
    "\n",
    "1) Remove highly correlated features\n",
    "2) Use principal component analysis (PCA)\n",
    "3) Ridge regression or Lasso regularization\n",
    "4) Combine correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJp6i_24iMD0"
   },
   "outputs": [],
   "source": [
    "### Q.15) How can feature selection improve model performance in regression analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsP-uNcsiMHy"
   },
   "outputs": [],
   "source": [
    "ans) Feature selection can:\n",
    "\n",
    "1) Reduce overfitting\n",
    "2) Improve model interpretability\n",
    "3) Decrease training time\n",
    "4) Remove noise from irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1ChgRroiMLD"
   },
   "outputs": [],
   "source": [
    "### Q.16) How is Adjusted R-squared calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuXi_koWiMQ7"
   },
   "outputs": [],
   "source": [
    "ans) Adjusted R-squared = 1 - [(1 - RÂ²)(n-1)/(n-k-1)]\n",
    "where n is sample size and k is number of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AH30Cl1iMUc"
   },
   "outputs": [],
   "source": [
    "### Q.17) Why is MSE sensitive to outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnxiI7YGiMXr"
   },
   "outputs": [],
   "source": [
    "ans) MSE squares errors, which magnifies large differences, making it particularly sensitive to outliers compared to metrics like MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TB0Z4xmuiMbN"
   },
   "outputs": [],
   "source": [
    "### Q.18) What is the role of homoscedasticity in linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94mLP7ewiMfL"
   },
   "outputs": [],
   "source": [
    "ans) Homoscedasticity means constant variance of residuals across all predictor values, ensuring reliable coefficient estimates and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cnQ396hiMjX"
   },
   "outputs": [],
   "source": [
    "### Q.19) What is Root Mean Squared Error (RMSE)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWxDfDYviMn0"
   },
   "outputs": [],
   "source": [
    "ans) RMSE is the square root of MSE, providing error measurement in the same units as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cqnw6lrviMsS"
   },
   "outputs": [],
   "source": [
    "### Q.20) Why is pickling considered risky?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLr3dQlTiMxL"
   },
   "outputs": [],
   "source": [
    "ans) Pickling can be risky because:\n",
    "\n",
    "1) Security vulnerabilities when unpickling untrusted files\n",
    "2) Version compatibility issues\n",
    "3) Platform dependency problems\n",
    "4) Potential for arbitrary code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5qkaAgXiM07"
   },
   "outputs": [],
   "source": [
    "### Q.21) What alternatives exist to pickling for saving ML models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7m6BAIviM4s"
   },
   "outputs": [],
   "source": [
    "ans) Alternatives include:\n",
    "\n",
    "1) ONNX format\n",
    "2) TensorFlow SavedModel\n",
    "3) Joblib\n",
    "4) Model-specific formats (like H5 for Keras)\n",
    "5) Custom serialization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOS9d66siM8V"
   },
   "outputs": [],
   "source": [
    "### Q.22) What is heteroscedasticity, and why is it a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZZygGHLiNAD"
   },
   "outputs": [],
   "source": [
    "ans) Heteroscedasticity is non-constant variance in residuals, causing:\n",
    "\n",
    "1) Inefficient parameter estimates\n",
    "2) Biased standard errors\n",
    "3) Invalid hypothesis tests\n",
    "4) Unreliable confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGEnxiCriNEd"
   },
   "outputs": [],
   "source": [
    "### Q.23) How can interaction terms enhance a regression model's predictive power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUjq7Ndsimsk"
   },
   "outputs": [],
   "source": [
    "ans)  Interaction terms can:\n",
    "\n",
    "1) Capture non-linear relationships\n",
    "2) Model feature dependencies\n",
    "3) Improve model flexibility\n",
    "4) Account for conditional effects between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKA2986UnEeC"
   },
   "source": [
    "**Practical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKSh1x3RinE8"
   },
   "outputs": [],
   "source": [
    "### Q.1) Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model using Seaborn's \"diamonds\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_CFAjMKnIZs"
   },
   "outputs": [],
   "source": [
    "ans) import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the diamonds dataset\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "\n",
    "# Preprocess the data: Select numeric features and drop rows with missing values\n",
    "diamonds = diamonds.select_dtypes(include=[np.number]).dropna()\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = diamonds.drop(columns=['price'])  # Exclude 'price' as it's the target\n",
    "y = diamonds['price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Visualize the distribution of residuals\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8b3sXgain4c"
   },
   "outputs": [],
   "source": [
    "### Q.2)  Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) for a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pfdLgr6iNHV"
   },
   "outputs": [],
   "source": [
    "ans) from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksSpZyeAisQ2"
   },
   "outputs": [],
   "source": [
    "### Q.3) Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOTbVfBpisg0"
   },
   "outputs": [],
   "source": [
    "ans) import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Linearity: Residuals vs Fitted\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title(\"Residuals vs Fitted\")\n",
    "plt.show()\n",
    "\n",
    "# Homoscedasticity\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title(\"Homoscedasticity Check\")\n",
    "plt.show()\n",
    "\n",
    "# Multicollinearity\n",
    "corr_matrix = X.corr()\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkZMOLgjiwjE"
   },
   "outputs": [],
   "source": [
    "### Q.4)  Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the performance of different regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXbCoq2yixM0"
   },
   "outputs": [],
   "source": [
    "ans) from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Cross-validation to evaluate performance\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "print(f\"Mean R-squared: {scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UI5M7e8TisrM"
   },
   "outputs": [],
   "source": [
    "### Q.5) Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and R-squared score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h94S_5MJnh9F"
   },
   "outputs": [],
   "source": [
    "ans) model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "r_squared = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Coefficients: {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "print(f\"R-squared: {r_squared}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBFt5eopniOT"
   },
   "outputs": [],
   "source": [
    "### Q.6)  Write a Python script that analyzes the relationship between total bill and tip in the tips dataset using simple linear regression and visualizes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFgEp6p6niYU"
   },
   "outputs": [],
   "source": [
    "ans) tips = sns.load_dataset('tips')\n",
    "\n",
    "# Simple Linear Regression\n",
    "X = tips[['total_bill']]\n",
    "y = tips['tip']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot\n",
    "sns.scatterplot(x=tips['total_bill'], y=tips['tip'])\n",
    "plt.plot(tips['total_bill'], y_pred, color='red')\n",
    "plt.title(\"Total Bill vs Tip\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHmwaXwHnieb"
   },
   "outputs": [],
   "source": [
    "### Q.7) Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the model to predict new values and plot the data points along with the regression line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSn2TqtOnijr"
   },
   "outputs": [],
   "source": [
    "ans) from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.title(\"Regression Line with Synthetic Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-1PDgT2nioR"
   },
   "outputs": [],
   "source": [
    "### Q.8)  Write a Python script that pickles a trained linear regression model and saves it to a file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erZ_uJlTnisk"
   },
   "outputs": [],
   "source": [
    "ans)  import pickle\n",
    "\n",
    "with open('linear_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "print(\"Model saved to 'linear_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rx11XBE9niw7"
   },
   "outputs": [],
   "source": [
    "### Q.9) Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the regression curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q88wI1XAni1y"
   },
   "outputs": [],
   "source": [
    "ans) from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Polynomial Regression\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
    "poly_model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = poly_model.predict(X)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.title(\"Polynomial Regression (Degree 2)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aY1baWgqni6e"
   },
   "outputs": [],
   "source": [
    "### Q.10) Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear regression model to the data. Print the model's coefficient and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcNveXJ5nxss"
   },
   "outputs": [],
   "source": [
    "ans) import numpy as np\n",
    "\n",
    "X = np.random.rand(100, 1) * 10  # Random values for X\n",
    "y = 3 * X.flatten() + np.random.randn(100) * 5  # Linear relation with noise\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"Coefficient: {model.coef_[0]}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgMZSaURnx5c"
   },
   "outputs": [],
   "source": [
    "### Q.11)   Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and compares their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYXMsZRNnyf8"
   },
   "outputs": [],
   "source": [
    "ans) degrees = [1, 2, 3, 4]\n",
    "for degree in degrees:\n",
    "    poly_model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression())\n",
    "    poly_model.fit(X, y)\n",
    "    y_pred = poly_model.predict(X)\n",
    "    plt.plot(X, y_pred, label=f\"Degree {degree}\")\n",
    "\n",
    "plt.scatter(X, y, color='blue', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial Regression Models\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tS1QgmRPnytE"
   },
   "outputs": [],
   "source": [
    "### Q.12)  Write a Python script that fits a simple linear regression model with two features and prints the model's coefficients, intercept, and R-squared score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rA0qW7Rmny-E"
   },
   "outputs": [],
   "source": [
    "ans) X = diamonds[['carat', 'depth']]\n",
    "y = diamonds['price']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "r_squared = model.score(X, y)\n",
    "\n",
    "print(f\"Coefficients: {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "print(f\"R-squared: {r_squared}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yFPOO2CnzPU"
   },
   "outputs": [],
   "source": [
    "### Q.13) Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the regression line along with the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZk5x6lnnze0"
   },
   "outputs": [],
   "source": [
    "ans) X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.title(\"Regression Line\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EC4IR7mEnzp0"
   },
   "outputs": [],
   "source": [
    "### Q.14) Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset with multiple features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkX98Aj_oBBL"
   },
   "outputs": [],
   "source": [
    "ans) import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
    "df = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])\n",
    "\n",
    "# Calculate VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = df.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VDGhwjVoBFU"
   },
   "outputs": [],
   "source": [
    "### Q.15) Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a polynomial regression model, and plots the regression curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vssz4taxoBIs"
   },
   "outputs": [],
   "source": [
    "ans) from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X**4 - 5 * X**3 + 2 * X**2 + 7 * X.flatten() + np.random.randn(100) * 100\n",
    "\n",
    "# Polynomial Regression\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree=4), LinearRegression())\n",
    "poly_model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = poly_model.predict(X)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(np.sort(X, axis=0), y_pred[np.argsort(X, axis=0)], color='red')\n",
    "plt.title(\"Polynomial Regression (Degree 4)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7ld9TVVLkrA"
   },
   "outputs": [],
   "source": [
    "### Q.16) Write a Python script that creates a machine learning pipeline with data standardization and a multiple linear regression model, and prints the R-squared score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnLs-fUVLk3f"
   },
   "outputs": [],
   "source": [
    "ans) from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train and evaluate\n",
    "pipeline.fit(X_train, y_train)\n",
    "r_squared = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"R-squared: {r_squared}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuoBzh8CLlH-"
   },
   "outputs": [],
   "source": [
    "### Q.17) Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the regression curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHsmy0CRLlSW"
   },
   "outputs": [],
   "source": [
    "ans) # Generate synthetic data\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = X**3 - 2 * X**2 + 5 * X.flatten() + np.random.randn(100) * 50\n",
    "\n",
    "# Polynomial Regression\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\n",
    "poly_model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = poly_model.predict(X)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(np.sort(X, axis=0), y_pred[np.argsort(X, axis=0)], color='red')\n",
    "plt.title(\"Polynomial Regression (Degree 3)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kmw4QuPJPWE_"
   },
   "outputs": [],
   "source": [
    "### Q.18) Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print the R-squared score and model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic2M0Y-QPWYW"
   },
   "outputs": [],
   "source": [
    "ans) # Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Results\n",
    "r_squared = model.score(X, y)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"Coefficients: {model.coef_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiQDpnfvPWjm"
   },
   "outputs": [],
   "source": [
    "### Q.19) Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the data points along with the regression line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivvJeBGfPWt2"
   },
   "outputs": [],
   "source": [
    "ans) # Generate synthetic data\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.flatten() + 5 + np.random.randn(100) * 2\n",
    "\n",
    "# Fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.title(\"Linear Regression Line\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bG_TRyAoPW7G"
   },
   "outputs": [],
   "source": [
    "### Q.20) Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's R-squared score and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owuhQjTYPXJO"
   },
   "outputs": [],
   "source": [
    "ans) # Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Results\n",
    "r_squared = model.score(X, y)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"Coefficients: {model.coef_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xY96lFlBPXUm"
   },
   "outputs": [],
   "source": [
    "### Q.21) Write a Python script that demonstrates how to serialize and deserialize machine learning models using joblib instead of pickling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuSQSHTxPXh2"
   },
   "outputs": [],
   "source": [
    "ans) from joblib import dump, load\n",
    "\n",
    "# Serialize model\n",
    "dump(model, 'linear_model.joblib')\n",
    "print(\"Model saved to 'linear_model.joblib'\")\n",
    "\n",
    "# Deserialize model\n",
    "loaded_model = load('linear_model.joblib')\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9SorrZFPXv-"
   },
   "outputs": [],
   "source": [
    "### Q.22) Write a Python script to perform linear regression with categorical features using one-hot encoding. Use the Seaborn tips dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3Nt-wZaPX82"
   },
   "outputs": [],
   "source": [
    "ans) from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load dataset\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "# One-hot encoding\n",
    "X = tips[['total_bill', 'sex', 'smoker']]\n",
    "y = tips['tip']\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(), ['sex', 'smoker'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "X_transformed = ct.fit_transform(X)\n",
    "\n",
    "# Fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X_transformed, y)\n",
    "\n",
    "# Results\n",
    "print(f\"Coefficients: {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWAUqJ3sPYIm"
   },
   "outputs": [],
   "source": [
    "### Q.23) Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and R-squared score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJMqy1x0PYUu"
   },
   "outputs": [],
   "source": [
    "ans) from sklearn.linear_model import Ridge\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X, y)\n",
    "\n",
    "# Linear Regression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X, y)\n",
    "\n",
    "# Results\n",
    "print(\"Ridge Regression Coefficients:\", ridge_model.coef_)\n",
    "print(\"Linear Regression Coefficients:\", linear_model.coef_)\n",
    "print(\"Ridge R-squared:\", ridge_model.score(X, y))\n",
    "print(\"Linear R-squared:\", linear_model.score(X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV-s6bIlPYlu"
   },
   "outputs": [],
   "source": [
    "### Q.24) Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idlyDCqFPY0e"
   },
   "outputs": [],
   "source": [
    "ans) from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(LinearRegression(), X, y, cv=5, scoring='r2')\n",
    "print(f\"R-squared scores: {scores}\")\n",
    "print(f\"Mean R-squared: {scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuL_AsdpP4q3"
   },
   "outputs": [],
   "source": [
    "### Q.25)  Write a Python script that compares polynomial regression models of different degrees and prints the R-squared score for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwau6p8YP5IW"
   },
   "outputs": [],
   "source": [
    "ans) degrees = [1, 2, 3, 4]\n",
    "for degree in degrees:\n",
    "    poly_model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression())\n",
    "    poly_model.fit(X, y)\n",
    "    r_squared = poly_model.score(X, y)\n",
    "    print(f\"Degree {degree} R-squared: {r_squared}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNHy1eZQ9YtaYMQOamTjxhM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
